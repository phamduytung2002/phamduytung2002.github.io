---
title: "NeuroMax: Enhancing Neural Topic Modeling via Maximizing Mutual Information and Group Topic Regularization."
author: Duy-Tung Pham
permalink: /publication/2024-11-12-neuromax
excerpt: '<strong>Duy-Tung Pham</strong>&#42;, Trang Nguyen&#42;, Tung Nguyen&#42;, Linh Ngo Van, Duc Anh Nguyen, Thien Huu Nguyen.'
# <br><br> TL;DR: This paper proposes a Neural Topic Modeling approach that i. leverages pretrained language models with low inference cost by maximizing mutual information, and ii. captures topic relationships through optimal transport.'
date: 2024-11-12
venue: 'Findings of the Association for Computational Linguistics: EMNLP'
paperurl: 'https://aclanthology.org/2024.findings-emnlp.457.pdf'
bibtexurl: '/files/1_neuromax.bib'
---
Recent advances in neural topic models have concentrated on two primary directions: the integration of the inference network (encoder) with a pre-trained language model (PLM) and the modeling of the relationship between words and topics in the generative model (decoder). However, the use of large PLMs significantly increases inference costs, making them less practical for situations requiring low inference times. Furthermore, it is crucial to simultaneously model the relationships between topics and words as well as the interrelationships among topics themselves. In this work, we propose a novel framework called NeuroMax (Neural Topic Model with Maximizing Mutual Information with Pretrained Language Model and Group Topic Regularization) to address these challenges. NeuroMax maximizes the mutual information between the topic representation obtained from the encoder in neural topic models and the representation derived from the PLM. Additionally, NeuroMax employs optimal transport to learn the relationships between topics by analyzing how information is transported among them. Experimental results indicate that NeuroMax reduces inference time, generates more coherent topics and topic groups, and produces more representative document embeddings, thereby enhancing performance on downstream tasks.
